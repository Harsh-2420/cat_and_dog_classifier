{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the helper functions\n",
    "# sigmoid function:\n",
    "\n",
    "def sigmoid(x):\n",
    "    s = 1/ (1 + np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising parameters\n",
    "# Creating vector with zeroes and initialising w and b param\n",
    "\n",
    "def initialise(v):\n",
    "    \"\"\"\n",
    "    Create a vector of np.shape() = (v, 1) and initialise b to 0\n",
    "    \"\"\"\n",
    "    w = np.zeros((v, 1))\n",
    "    b = 0 \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward and Backward Propogation\n",
    "\n",
    "def propogation(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement cost function and its gradient for the propogation\n",
    "    \"\"\"\n",
    "    m = X.shape[1] # m is used to average out the cost function and so it is the total number of columns in X\n",
    "    \n",
    "    # Forward Propogation\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    cost = (-1 / m) * np.sum(Y * np.log(A) + (1 - Y)* np.log(1 - A)) \n",
    "    \n",
    "    # Backward Propogation\n",
    "    dw = 1/m * np.dot(X, (A - Y).T)\n",
    "    db = i/m * np.sum(A - Y)\n",
    "    \n",
    "    grads = {\"dw\": dw, \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization \n",
    "\n",
    "def optimize(w, b, X, Y, num_iter, learning_rate, print_cost):\n",
    "    costs = []\n",
    "    for i in range(num_iter):\n",
    "        grads, cost = propogation(w, b, X, Y)\n",
    "        dw = grads['dw']\n",
    "        db = grads['db']\n",
    "        \n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "        params = {'w': w, 'b': b}\n",
    "        grads = {\"dw\": dw, \"db\": db}\n",
    "\n",
    "        return params, grads, costs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the initial prediction model using our new w and b\n",
    "\n",
    "def predict(w, b, X):\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Vector A is to predict the probability\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        Y_prediction[0, i] = 1 if A[0, i] > 0.5 else 0\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
